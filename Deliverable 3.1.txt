CS 6343.001
Team 1 Project Plan






































Deliverable 3.1 Report
Team Members: Balaguru Sethuraman, Cole Oftedahl, Kevin Wilkes, Thi Vu, Thien Nguyen
Introduction
This project’s purpose is to explore the use of Kubernetes and the tools it provides for deploying cloud-based solutions. This entails creating a distributed workflow to execute in multiple containers and test the effectiveness of different cloud mechanisms, such as cloud architecture, component scheduling, and resource allocation, for the given workflow. 
The background knowledge needed for the project largely comes from the required reading links provided in the project document. To briefly describe this background information, Kubernetes provides a range of mechanisms to manage cloud deployments. These mechanisms include things like node and pod affinity and anti-affinity, node taint and pod toleration, deployment topology, scheduler configurations and algorithms, and horizontal autoscalers. 
This project will utilize all the aforementioned background knowledge by testing the performance of different configurations for a cloud deployment. First, the workflow of the deployment must be created, which requires work not associated with Kubernetes. Then, the deployment must be created and the components must be connected using Kubernetes mechanisms like Services and Ingresses. Finally, the above topics come into play in optimizing the performance of the deployment. 
Proposed Workflow Ideas & Final Choice
Various ideas were proposed by the team for a workflow. Below are the ideas compiled for the potential processes. 


* Video Processing:
   * Upload/Stream raw video files
   * Make multiple resolutions (1080p, 720p, 480p)
   * Apply filters/effects
   * Create thumbnails
   * Store processed files in DB
* Machine Learning:
   * Data preprocessing (normalization, etc.)
   * Training multiple models in parallel
   * Etc.
* Image Processing
   * Object detection
* Web Scraping + Data Analysis
   * Similar to video processing


Our team chose to use a workflow related to video processing in the context of a security system, such as a CCTV system. The system will have an input component where video data is uploaded, processing components to resize the images from the video by way of modifying the resolutions and performing object recognition on the images to detect potential items in the video and analyze potential significant frames, such as when a person enters the frame in an otherwise static environment, and finally an output component in the form of a streaming server which allows a user to view the processed video content. This system may also include a supporting data storage component, either in the form of a database or a simple volume where files can be stored. 
Description of Workflow
This section details each component’s purpose and proposed implementation or technology to use. To start with, the overall view of the system, its components, and the interactions between components to form the workflow shall be described. 
The component’s functionalities are represented in the following class diagram. Each class in the diagram represents a component of the system, except for the StoredVideo and Stored Images classes, which represent data transfer formats. The public methods associated with each class in the diagram represent functionality that will be accessible through a public route, as each component will be encapsulated as an HTTP server. 
Figure 1: Class Diagram
  

The interactions between the components occur in two distinct use cases. One use case is uploading and processing images or a video. The following sequence diagram shows the flow of interactions, with the parameters and returns indicating the flow of data. 
Figure 2: Upload Image Sequence Diagram
  

The second use case is viewing the processed videos. The processed files are stored on the StorageServer, so this use case simply must retrieve them. 
Figure 3: View Video Sequence Diagram
  





Next, each component will be described individually in the following subsections. Each subsection listed below is intended to be deployed as its own Kubernetes pod, potentially with the system containing multiple replicas of each pod. 
1. Input Server
The Input will be managed by a Kafka container that produces frames from any YouTube Livestream. This input data will be formatted to enable future components to process it correctly from the YouTube API formatting. This will be done in Python as it can be easily integrated with Kafka to become a content producer. Data processing in the future is compatible with PySpark and won’t conflict with future components.
Explain how the code works and describe any problems faced when designing the code.
The current program streams and displays a live YouTube video directly using yt_dlp, ffmpeg, and OpenCV without downloading it first. It begins by using yt_dlp to extract the direct video stream URL from a YouTube link. Then, ffprobe retrieves the video’s width and height to determine the frame size. ffmpeg is launched as a subprocess to convert the video stream into raw frame data in real time, which is continuously read into NumPy arrays. Currently, it sends the compatible RGB array matrices of the images to Kafka, which will be able to send to future components for data processing.
Some problems faced while implementing the code were the need to utilize ffmpeg due to the awkward size that the YouTube API provided. Some on-site image reprocessing needed to be done in order for OpenCV to display. This was required so future programs could process the images created.
The code:
YoutubeStreamCode
2. Video Processing - Rescaling
The image rescaling processing stage will be performed using the OpenCV library. As OpenCV is available in many languages, this component could be implemented in a variety of ways, but will likely be implemented in Python. This library contains functions for automatic rescaling of images using various interpolation methods. Some preset output image qualities will include 720p, 1080p, and 1440p, which are all standard video resolutions. Determining which interpolation method to use depends on whether the image is being upsampled or downsampled (increasing or decreasing the resolution). For downsampling, the interpolation method will be Inter-Area, and for upsampling, the interpolation method will be Inter-Cubic. Processed images will be stored in the data store component, grouped together by the video they belong to, for easy retrieval and streaming of preset-quality videos. 
Since the processing is very easy to implement using the OpenCV library, it will be custom-implemented rather than downloading a pre-existing component. The processing function will take an image file as input, read the file in OpenCV format, perform the rescaling to each resolution using the one-line OpenCV built-in function, and output all the rescaled images. This function will be wrapped in an HTTP server with a single POST endpoint to which an image can be submitted for processing. All the code for this is implemented in Python, with the HTTP server using Flask. Output will need to be routed once the modules are connected, but for local development, the processed images are all simply displayed using matplotlib.pyplot. 
The following code listing shows the processing needed for this component, providing the custom-implemented Python code that was described above. 
Listing 1: Component 2 Custom Code
@app.route('/upload', methods=['POST'])
@cross_origin(origin="*")
def upload_image():
  # Check if the request contains a file
  if 'image' not in request.files:
    return jsonify({'error': 'No image file provided'}), 400


  file = request.files['image']


  # Check if the file is valid
  if file.filename == '':
    return jsonify({'error': 'No selected file'}), 400


  if file and allowed_file(file.filename):
    # read file data
    file_bytes = np.frombuffer(file.read(), np.uint8)
    img_data = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)


    # perform image processing
    height, width, _ = img_data.shape
    starting_size =  (height, width)
    max_starting_dim = max(starting_size[0], starting_size[1])
    img_resized_256 = cv2.resize(img_data, (256,256), interpolation=cv2.INTER_AREA if (256 < max_starting_dim) else cv2.INTER_CUBIC)
    img_resized_720 = cv2.resize(img_data, (1280,720), interpolation=cv2.INTER_AREA if (1280 < max_starting_dim) else cv2.INTER_CUBIC)
    img_resized_1080 = cv2.resize(img_data, (1920,1080), interpolation=cv2.INTER_AREA if (1920 < max_starting_dim) else cv2.INTER_CUBIC)
    img_resized_1440 = cv2.resize(img_data, (2560,1440), interpolation=cv2.INTER_AREA if (2560 < max_starting_dim) else cv2.INTER_CUBIC)
In addition to the image resizing occurring, the server implements simple error handling by checking that the file name and file type are valid before trying to read and process the image. 
The following listing provides the Dockerfile contents, which were produced by the “docker init” command run on the image processing server code. 
Listing 2: Component 2 Dockerfile
ARG PYTHON_VERSION=3.13.2
FROM python:${PYTHON_VERSION}-slim as base
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
WORKDIR /app
ARG UID=10001
RUN adduser \
    --disabled-password \
    --gecos "" \
    --home "/nonexistent" \
    --shell "/sbin/nologin" \
    --no-create-home \
    --uid "${UID}" \
    appuser
RUN --mount=type=cache,target=/root/.cache/pip \
    --mount=type=bind,source=requirements.txt,target=requirements.txt \
    python -m pip install -r requirements.txt
USER appuser
COPY . .
EXPOSE 8000
CMD python src/cloud_proj_1/server.py
________________
3. Video Processing - Object Detection
The object detection component analyzes frames from the uploaded video to identify and classify key objects of interest, such as people, vehicles, and other items that indicate activity in the scene. This component is critical for the security system workflow as it enables automated monitoring and event detection
This component is implemented using YOLO12 (You Only Look Once), the latest deep learning model optimized for real-time object detection. YOLO12-nano was specifically chosen for its excellent balance of speed, accuracy, and resource efficiency, making it ideal for processing video frames in near real-time on CPU-only infrastructure. The component is built as a Flask-based REST API server that can receive images via HTTP POST requests and return detection results in JSON format.
The server provides several endpoints:
- POST /detect: Processes a single image and returns detected objects with bounding boxes, confidence scores, and an annotated image
- POST /detect/batch: Processes multiple images in a single request for improved throughput
- GET /health: Health check endpoint for Kubernetes liveness and readiness probes
- GET /info: Returns model information and configuration details
The core detection logic is implemented in Python using the Ultralytics YOLO library, OpenCV for image processing, and Flask for the HTTP server. The model weights (yolo12n.pt) are pre-downloaded during Docker image build to eliminate startup delays. The following code listing shows the main detection endpoint:


Listing 3: Component 3 Code
  
  

The code performs several key operations:
* Validates the incoming image file
* Decodes the image using OpenCV
* Runs YOLOv11 inference with configurable confidence and IOU thresholds
* Extracts bounding boxes, class labels, and confidence scores
* Generates an annotated image with drawn bounding boxes
* Returns JSON response with all detection metadata
The component is containerized using Docker for easy deployment and scalability. The Dockerfile uses Python 3.11-slim as the base image and includes all necessary system dependencies for OpenCV and PyTorch.
Listing 4: Component 3 Dockerfile
  
  











Key Dockerfile Features:
* Uses a multi-stage build for optimization
* Installs system dependencies required for OpenCV and deep learning libraries
* Creates a non-privileged user for security
* Includes a health check for Kubernetes integration
* Exposes port 8000 for HTTP traffic
* Sets configurable environment variables for detection parameters


Implementation challenges and solutions included:
1. Model Download Delays: Initial pod startup was delayed due to automatic download of YOLO12 model weights on first run. This was resolved by pre-downloading the model (yolo12n.pt) during Docker image build using wget, reducing startup time from 120+ seconds to under 40 seconds.
2. Memory Constraints: Initial testing with high-resolution images revealed insufficient memory allocation causing pod restarts. Solution involved using YOLO12-nano model variant (smallest footprint) and setting memory request to 1Gi with a limit of 3Gi.
3. K3s Image Distribution: Unlike Docker Swarm, K3s uses containerd requiring a different image distribution workflow. Implemented docker save/k3s ctr images import pipeline to distribute images from master node to worker nodes.
4. Health Check Timing: Initial startup probes failed due to YOLO model initialization time. Configured startup probe with 10s delay, 10s period, and 12 failure threshold (120s total) to allow sufficient initialization time while maintaining fast failure detection after startup.
4. Output Server
        The output and streaming component will make the processed video content available to clients in real time or on demand by packaging video files into streamable formats and delivering them efficiently. This service will be implemented using the HTTP streaming protocol, such as HLS or MPEG-DASH, which generate playlist files and small video segments that clients can fetch progressively. The pod running this service will be memory optimized to handle high-throughput video delivery and concurrent client connections, with Kubernetes managing multiple replicas to provide scalability and reliability under varying loads. The processed video files will be stored directly in the file system, while unique identifiers and associated metadata (e.g., timestamps, bounding boxes) will be stored in a PostgreSQL database. An Ingress Controller will expose the service externally through secure HTTP(S) endpoints.
        The code will be implemented using the Clappr media player extension to display the processed video content. It allows users to select options from a dropdown list of streaming services to send to the Input Service, starting the process of Video Processing and Object Detection to retrieve. The HTML file will simply display the media player, where the source of the player will be connected to an HLS playlist. A Dockerfile will be created to define how to package our application for Kubernetes, along with service and ingress configurations to connect the container to the outside world. Ingress will route all external traffic from our URL to our output service within Kubernetes.
        The following listing provides the Dockerfile contents to build and run locally for testing purposes, but later on, it will interact with other components.
Listing 5: Component 4 Dockerfile
  

5. DB Server
We will use a combination of PostgreSQL and the file system. The videos will be stored as files, and the identifier will be stored in the SQL table, which can be used for the retrieval of specific data. We will use the Network File System (NFS) for the file storage and add Postgres to the NFS to access the data. The Input server will send a request to the NFS to store the file, and the models retrieve the data from this server. All data storage and manipulation takes place in this server.
The server pod will use a Stateful service type, and the data allocation is extended whenever the usage is greater than 85%. The allocation is done manually, and monitoring is done using Prometheus to keep track of the disk usage. The following images show the NFS setup in the remote servers given to our group. This is a basic NFS setup and how the mounting of the folder works. PostgreSQL will be used in the NFS server pod and will store all the SQL data and files.  
Listing 6: Component 5 - Client mounting the NFS
  



Listing 7: Component 5 - NFS Server
  
   


  

Resource Allocation Plan
The following items have been identified as potential constraints on the deployment architecture and pod execution. 
* Video Processing Nodes: Components 2 and 3 will be multi-replica and load-balanced (request-based balancing)
* Components 2 and 3 will have pod anti-affinity with themselves and each other by sharing a tag such as CPUIntensive. This anti-affinity will be used because these components are the most computationally expensive components of the system, so spreading these pods out among separate nodes will help to improve the performance of the system. 
* Components 2, 3, and 4 will use horizontal autoscaling based on a desired average CPU utilization of 70% of the limit for each pod. Initially, the idea was to use the incoming request rate, but it will be more effective to use the standard metric of CPU utilization, which is a better measure for when to autoscale, as each request may require more or less computation depending on the amount of data needed to process in the request. 
* Component 3 uses a pre-trained YOLO12-nano neural network to detect objects in images. Initial plan included GPU scheduling, but deployment uses CPU-only configuration with optimized YOLO12-nano model for broader VM compatibility. Component 3 pods are scheduled to worker node csa-6343-104.utdallas.edu using node selector (workload=object-detection) to ensure consistent placement and resource allocation. 
Each component will have the pods configured for a certain resource request and limit. The following items provide those resource values for each component. 
* Component 1: CPU limit of 250m, CPU request of 100m
* Component 2: CPU limit of 1000m, CPU request of 200m
* Component 3: CPU limit of 2 cores, CPU request of 500m, Memory request: 1Gi, Memory limit: 3Gi (CPU-only, no GPU)
* Component 4: CPU limit of 500m, CPU request of 250m
* Component 5:  Disk limit of 8Gi


The above constraints are necessary because certain components will use similar resources, and thus should not be deployed together, as this would under-utilize other resources on the same node. For example, placing both processing components (Components 2 and 3) on the same physical node will cause the node to heavily utilize the CPU, while memory is less utilized, and would be better spent hosting the streaming component (Component 4), which does less actual processing of the data. 
Using the above constraints, the workflow is predicted to perform better than using the default Kubernetes placement options. In this optimized configuration, the number of replicas of each component to deploy in a single workflow is as follows:
* Component 1: 1 replica for each client [TODO]
* Component 2: 2 replicas
* Component 3: 2 replicas (deployed on node 104, with HPA configured for dynamic scaling based on CPU utilization)
* Component 4: 2 replicas
* Component 5:  1 replica (limit because db is not dynamically scalable using NFS)


In addition to the replicas for each component, the entire workflow should be deployed 3 times to generate sufficient load. Note that as of 11/05, the team only has access to 3 working VMs, not 4, since csa-6343-101 went down (TA was notified and is working on it), so the workload does not need to be as large to stress the system. Each workflow deployed will be identical in makeup to ensure that each component’s resource constraints are consistent. 
Considering the optimized configuration developed above, there are ways that even this configuration could be found lacking in performance. The main way in which that could occur is through a bottleneck at the database server. Since the database server is configured using NFS, not a DFS system, all requests to the database server are expected to route to one physical node. Thus, as the workflow continues to scale outwards or more workflows are deployed, the number of physical nodes supporting the database remains constant, which could result in a bottleneck. Thus, using both the Kubernetes default configuration and the developed configuration, the workflow’s workload could become unbalanced, with the database constantly serving requests while other components may idle occasionally while waiting because of the high traffic the database is trying to serve. 
Deployment Plan
The team decided collectively to use K3s as the Kubernetes solution to deploy on the VMs. The individual components have all been deployed and are now being integrated into a cluster for a full workflow. 
[TODO - update your components’ progress in this section]
* Component 1: Developed custom code to interface with YouTube and containerized it. The next step is to deploy the Docker container using multiple VMs, gathering data from different YouTube streams. Afterwards, connect the data input to video processing.
* Component 2: Developed custom server code and containerized it. Tested that containerized deployment worked locally. Experienced an issue with connecting to the VPN, and worked with the professor to resolve it. Deployed containers in the cluster with a service to make them accessible, and a configmap to easily modify the destination URL for the data pipeline. 
* Component 3: Developed a custom object detection server using YOLO12-nano and containerized it with pre-downloaded model weights. Successfully deployed to team K3s cluster on worker node csa-6343-104.utdallas.edu with 2 replica pods. Resolved model download delays by embedding model in Docker image (startup reduced from 120s to 40s). Resolved memory constraints through YOLO12-nano model selection and optimized resource allocation (1Gi request, 3Gi limit). Implemented CPU-only deployment eliminating GPU dependencies while maintaining acceptable performance for real-time processing. Service accessible cluster-wide at http://object-detection-service:8000 with endpoints for health checks, single/batch detection, and model info. Configured with horizontal pod autoscaling (HPA) based on 70% CPU utilization target.
* Component 4: Developed custom output server code and containerized it. Tested that containerized deployment worked locally. Deployed containers in the cluster successfully, but are experiencing issues getting access to the service through my browser using ingress to test the service. Currently fixing my custom code so it can display the application correctly when accessed from the browser.
* Component 5: Set up NFS on the remote server. Will work on deploying a PostgreSQL service on the server and use it through the network. 
Experiments and Results
Clients will initiate processing by either submitting a URL to the input server or uploading a video file. This is accessible through a web-based front end exposed from the input server to clients. For submitting a URL, the URL will provide a video stream, from which images will be recorded and processed in the system. If the client uploads a video file, once more, the images that comprise the video will be individually processed in the system. 
Experiments have not yet been performed, but in developing the resource allocation plan, different execution configurations were considered, which can be explored in the experiments to see the impact of scheduling and placement. First, the default placement will be used, where no placement constraints are specified for the nodes. The performance of the system will be measured for this default configuration and then compared against the performance of the developed configuration, which is specified in the Resource Allocation Plan above. In comparing the performance of these configurations (given the same input to process), the impact of scheduling will be recorded and analysed. 
In performing the experiments, the response time and throughput will largely be used as the metrics. First, an experiment can be performed where only one large video file is submitted, and the response time for the two configurations is measured. Next, several large video files can be submitted to each system, and the throughput of the system can be measured while processing the files. In this manner, the two configurations’ performance will be measured through various experiments. 
The results of these experiments will be recorded in a table for clarity and will be presented in a graphical format in the final report to easily offer insights into the data trends. The performance of the workflow for each input will be charted to make a line (in the line graph), and each configuration of the workflow will have its own line to offer comparisons side-by-side of the different workflow configuration performances. 
Workload Distribution
This table lists what components each member researched and described solutions for in this milestone. 
Member Name
	Member NETID
	Contributions
	Balaguru Sethuraman
	bxs230069
	DB Server component description
	Cole Oftedahl
	cxo220001
	Video Processing - Rescaling component description
	Kevin Wilkes
	kdw190001
	Input Server component description
	Thi Vu
	tav180002
	Output Server component description
	Thien Nguyen
	dxn210021
	Video Processing - Object Detection component description
	In addition to the individual contributions listed in the table, the team collaboratively found ideas for the project workflow and developed team sections of the report as a team. All team members were active in contributing equally to the deliverable. 
The next table lists some of the team’s weekly accomplishments. 
Date of Week’s End
	Accomplishments
	09/26
	Researched Kubernetes basic concepts and deployed our own apps locally. 
	10/03
	Decided on workflow and component ideas, and detailed internal processing needed for each component. Identified potential data source and integration method from data to the deployed system. 
	10/10
	Expanded on Kubernetes tools utilization by detailing pod resource requirements and application of placement algorithms to enhance performance. Decided on a Kubernetes solution to use in VMs and started working on getting images to the VMs. 
	10/17
	Continued working on developing and deploying individual containerized applications on VMs. 
	10/24
	Finished deploying individual containerized apps on VMs. Looked into deployment constraints, how to integrate apps, and resource allocation mechanisms to optimize the system. Also looked into how to unbalance the system workload to reduce performance. 
	10/31
	Configured a Kubernetes cluster with a master to allow integration and single-point deployment of containers. 
	11/07
	Developed report for milestone 3.1. Deployed components to the cluster.